{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import query_sql as sql\n",
    "import conn\n",
    "import numpy as np\n",
    "from excel_beautify import beautify_excel\n",
    "from send_email import send_bulk_emails_with_attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置日报查询范围\n",
    "end_day = str(pd.Timestamp.now().date()-pd.Timedelta(days=1))+\" 23:59:59\"\n",
    "start_day = str((pd.Timestamp.now()- pd.Timedelta(days=1)).replace(day=1).strftime('%Y-%m-%d')) +' 00:00:00'\n",
    "loan_end_day = end_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置工作目录为脚本所在目录\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "# 读取配置文件获取电销业务组\n",
    "with open('config.yaml','r',encoding='UTF-8') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "group = \",\".join(f\"'{cf}'\" for cf in config['philippines']['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-25 23:59:59 2024-09-01 00:00:00 2024-09-25 23:59:59\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT\n\tDATE(call_detail.dial_time) `call_date`,\n\tcall_detail.dunner_id `user_id`,\n\tSUM( call_detail.talk_duration ) `talk_duration`,\n\tCOUNT(IF( call_detail.talk_duration > 0, 1, NULL )) `call_times` \nFROM\n-- 通话明细 start-- \n\t(\n\tSELECT\n\t\ttche.dunner_id,\n\t\ttch.dial_time,\n\t\ttch.talk_duration \n\tFROM\n\t\tods_fox_tel_sale_call_history tch\n\t\tLEFT JOIN ods_fox_tel_sale_call_history_extend tche ON tch.id = tche.source_id \n\tWHERE\n\t( tch.dial_time >= '2024-09-01 00:00:00' AND tch.dial_time <= '2024-09-25 23:59:59' )) `call_detail` \n\t-- 通话明细 end-- \nGROUP BY\n\tDATE(call_detail.dial_time),\n\tcall_detail.dunner_id\n': (1064, 'Build Exec OlapScanNode fail, scan info is invalid')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pymysql\\cursors.py:153\u001b[0m, in \u001b[0;36mCursor.execute\u001b[1;34m(self, query, args)\u001b[0m\n\u001b[0;32m    151\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmogrify(query, args)\n\u001b[1;32m--> 153\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executed \u001b[38;5;241m=\u001b[39m query\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pymysql\\cursors.py:322\u001b[0m, in \u001b[0;36mCursor._query\u001b[1;34m(self, q)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_result()\n\u001b[1;32m--> 322\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_get_result()\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pymysql\\connections.py:558\u001b[0m, in \u001b[0;36mConnection.query\u001b[1;34m(self, sql, unbuffered)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_command(COMMAND\u001b[38;5;241m.\u001b[39mCOM_QUERY, sql)\n\u001b[1;32m--> 558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_affected_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_query_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43munbuffered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munbuffered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_affected_rows\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pymysql\\connections.py:822\u001b[0m, in \u001b[0;36mConnection._read_query_result\u001b[1;34m(self, unbuffered)\u001b[0m\n\u001b[0;32m    821\u001b[0m     result \u001b[38;5;241m=\u001b[39m MySQLResult(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 822\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pymysql\\connections.py:1200\u001b[0m, in \u001b[0;36mMySQLResult.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1200\u001b[0m     first_packet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_packet\u001b[38;5;241m.\u001b[39mis_ok_packet():\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pymysql\\connections.py:772\u001b[0m, in \u001b[0;36mConnection._read_packet\u001b[1;34m(self, packet_type)\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39munbuffered_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 772\u001b[0m     \u001b[43mpacket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m packet\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pymysql\\protocol.py:221\u001b[0m, in \u001b[0;36mMysqlPacket.raise_for_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno =\u001b[39m\u001b[38;5;124m\"\u001b[39m, errno)\n\u001b[1;32m--> 221\u001b[0m \u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_mysql_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pymysql\\err.py:143\u001b[0m, in \u001b[0;36mraise_mysql_exception\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    142\u001b[0m     errorclass \u001b[38;5;241m=\u001b[39m InternalError \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m OperationalError\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m errorclass(errno, errval)\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (1064, 'Build Exec OlapScanNode fail, scan info is invalid')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m attendance_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(sql\u001b[38;5;241m.\u001b[39mattendance_days_sql\u001b[38;5;241m.\u001b[39mformat(start_day,end_day,loan_end_day),conn_link)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 每日过程数据\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m process_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43mend_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloan_end_day\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconn_link\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 离职日期查询\u001b[39;00m\n\u001b[0;32m     10\u001b[0m dimission_date_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(sql\u001b[38;5;241m.\u001b[39mdimission_date_sql\u001b[38;5;241m.\u001b[39mformat(end_day),conn_link)\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT\n\tDATE(call_detail.dial_time) `call_date`,\n\tcall_detail.dunner_id `user_id`,\n\tSUM( call_detail.talk_duration ) `talk_duration`,\n\tCOUNT(IF( call_detail.talk_duration > 0, 1, NULL )) `call_times` \nFROM\n-- 通话明细 start-- \n\t(\n\tSELECT\n\t\ttche.dunner_id,\n\t\ttch.dial_time,\n\t\ttch.talk_duration \n\tFROM\n\t\tods_fox_tel_sale_call_history tch\n\t\tLEFT JOIN ods_fox_tel_sale_call_history_extend tche ON tch.id = tche.source_id \n\tWHERE\n\t( tch.dial_time >= '2024-09-01 00:00:00' AND tch.dial_time <= '2024-09-25 23:59:59' )) `call_detail` \n\t-- 通话明细 end-- \nGROUP BY\n\tDATE(call_detail.dial_time),\n\tcall_detail.dunner_id\n': (1064, 'Build Exec OlapScanNode fail, scan info is invalid')"
     ]
    }
   ],
   "source": [
    "print(end_day,start_day,loan_end_day)\n",
    "# 每日分案对应的业绩数据\n",
    "conn_link = conn.conn_ph()\n",
    "performance_df = pd.read_sql(sql.performance_sql.format(start_day,end_day,loan_end_day),conn_link)\n",
    "# 每日是否出勤\n",
    "attendance_df = pd.read_sql(sql.attendance_days_sql.format(start_day,end_day,loan_end_day),conn_link)\n",
    "# 每日过程数据\n",
    "process_df = pd.read_sql(sql.process_sql.format(start_day,end_day,loan_end_day),conn_link)\n",
    "# 离职日期查询\n",
    "dimission_date_df = pd.read_sql(sql.dimission_date_sql.format(end_day),conn_link)\n",
    "# 首次上线日期查询\n",
    "first_online_df = pd.read_sql(sql.first_online_sql.format(start_day,end_day,loan_end_day),conn_link)\n",
    "# 应出勤天数\n",
    "required_attendance_df = pd.read_sql(sql.required_attendance_days.format(start_day,end_day),conn_link)\n",
    "# 当月所有排班人员数据查询\n",
    "current_month_first_scheduled_df = pd.read_sql(sql.current_month_first_scheduled.format(start_day,end_day,group),conn_link)\n",
    "# uo表每日架构\n",
    "organization_df = pd.read_sql(sql.organization_sql.format(start_day,end_day,group),conn_link)\n",
    "# 分案表每日架构\n",
    "assign_organization_df = pd.read_sql(sql.assign_organization.format(start_day,end_day),conn_link) \n",
    "# 工号及是否计入流失标志\n",
    "no_count_churn_df = pd.read_sql(sql.no_count_churn,conn_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_organization_by_date (organization_df,group_col1,group_col2,value):\n",
    "    \"\"\"\n",
    "    获取每个组按时间列最新的数据。\n",
    "    参数:\n",
    "    - organization_df: pandas DataFrame,包含要操作的数据\n",
    "    - group_col1: str,分组列的名称\n",
    "    - group_col2: str,分组列的名称\n",
    "    - value: str,时间列的名称\n",
    "    返回:\n",
    "    - organization_latest_df: 包含每个组最新数据的行\n",
    "    \"\"\"\n",
    "    organization_df[value] = pd.to_datetime(organization_df[value])\n",
    "    latest_indices = organization_df.groupby([group_col1,group_col2])[value].idxmax()\n",
    "    organization_latest_df = organization_df.loc[latest_indices]\n",
    "    return organization_latest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按时间获取最新同业务组员工架构\n",
    "latest_uo_df = get_latest_organization_by_date(organization_df,group_col1 = 'group', group_col2='user_id', value='uo_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每日架构匹配分案放款数据\n",
    "organization_df['uo_date'] = pd.to_datetime(organization_df['uo_date'])\n",
    "performance_df['assign_date'] = pd.to_datetime(performance_df['assign_date'])\n",
    "\n",
    "get_performance_df = pd.merge(\n",
    "    organization_df,\n",
    "    performance_df,\n",
    "    left_on=['user_id','uo_date'],\n",
    "    right_on=['user_id','assign_date'],\n",
    "    how='left'\n",
    "    ).drop(columns=['assign_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每日架构匹配每日过程数据\n",
    "process_df['call_date'] = pd.to_datetime(process_df['call_date'])\n",
    "get_process_df = pd.merge(\n",
    "    organization_df,\n",
    "    process_df,\n",
    "    left_on=['user_id','uo_date'],\n",
    "    right_on=['user_id','call_date'],\n",
    "    how='left'\n",
    "    ).drop(columns=['call_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每日架构匹配每日出勤数据\n",
    "attendance_df['work_day'] = pd.to_datetime(attendance_df['work_day'])\n",
    "get_attendance_df = pd.merge(\n",
    "    organization_df,\n",
    "    attendance_df,\n",
    "    left_on=['user_id','uo_date'],\n",
    "    right_on=['user_id','work_day'],\n",
    "    how='left'\n",
    "    ).drop(columns=['work_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匹配上线日期判断新老人上线天数\n",
    "first_online_df['first_online_day'] = pd.to_datetime(first_online_df['电销首次上线日期'])\n",
    "get_online_frist_df = first_online_df\n",
    "get_attendance_df = pd.merge(\n",
    "    get_attendance_df,\n",
    "    get_online_frist_df,\n",
    "    left_on='user_id',\n",
    "    right_on='坐席ID',\n",
    "    how= 'left'\n",
    ").drop(columns=['电销首次上线日期','坐席ID'])\n",
    "get_attendance_df['newly_online'] = get_attendance_df['uo_date'] <= (get_attendance_df['first_online_day'] + pd.Timedelta(days=29))\n",
    "get_attendance_df['old_online'] = get_attendance_df['uo_date'] > (get_attendance_df['first_online_day'] + pd.Timedelta(days=29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算员工数据 start\n",
    "# 业绩数据架构替换为业务组内最新架构\n",
    "staff_performance = get_performance_df.drop(columns=['uo_date','director','team_leader','user_name'])\n",
    "staff_performance = pd.merge(staff_performance,latest_uo_df,left_on=['user_id','group'],right_on=['user_id','group'],how='left')\n",
    "\n",
    "staff_performance = staff_performance.groupby(['group','director','team_leader','user_id']).agg(\n",
    "    assign_sum = ('include_assign','sum'),\n",
    "    loan_sum = ('include_loan','sum'),\n",
    "    application_sum = ('include_application','sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每日过程数据替换为业务组内最新架构\n",
    "staff_process = get_process_df.drop(columns=['uo_date','director','team_leader','user_name'])\n",
    "staff_process = pd.merge(staff_process,latest_uo_df,left_on=['user_id','group'],right_on=['user_id','group'],how='left')\n",
    "staff_process = staff_process.groupby(['group','director','team_leader','user_id']).agg(\n",
    "    call_times_sum = ('call_times','sum'),\n",
    "    talk_duration_sum = ('talk_duration','sum')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每日出勤替换为业务组内最新架构\n",
    "staff_attendance = get_attendance_df.drop(columns=['uo_date','director','team_leader','user_name'])\n",
    "staff_attendance = pd.merge(staff_attendance,latest_uo_df,left_on=['user_id','group'],right_on=['user_id','group'],how='left')\n",
    "staff_attendance = staff_attendance.groupby(['group','director','team_leader','user_id']).agg(\n",
    "    attendance_sum = ('attendance_status','sum'),\n",
    "    newly_sum = ('attendance_status',lambda x: (staff_attendance.loc[x.index,'newly_online'] & x == 1).sum()),\n",
    "    old_sum = ('attendance_status',lambda x: (staff_attendance.loc[x.index,'old_online'] & x == 1).sum())\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 员工数据合并\n",
    "staff_uo_lastest = latest_uo_df.drop(columns=['uo_date'])\n",
    "合并业绩数据 = pd.merge(staff_uo_lastest,staff_performance.drop(columns=['team_leader','director']),left_on = ['user_id','group'],right_on = ['user_id','group'],how='left')\n",
    "合并过程数据 = pd.merge(合并业绩数据,staff_process.drop(columns=['team_leader','director']),left_on = ['user_id','group'],right_on = ['user_id','group'],how='left')\n",
    "合并出勤天数 = pd.merge(合并过程数据,staff_attendance.drop(columns=['team_leader','director']),left_on = ['user_id','group'],right_on = ['user_id','group'],how='left')\n",
    "匹配上线日期 = pd.merge(合并出勤天数,first_online_df.drop(columns='电销首次上线日期'),left_on='user_id',right_on='坐席ID',how='left')\n",
    "匹配离职日期 = pd.merge(匹配上线日期,dimission_date_df,on='user_id',how='left')\n",
    "# 员工基础数据\n",
    "staff_base = 匹配离职日期.drop(columns='坐席ID')\n",
    "staff_base['dimission_date'] = pd.to_datetime(staff_base['dimission_date'])\n",
    "staff_base['first_online_day'] = pd.to_datetime(staff_base['first_online_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断是否新人\n",
    "staff_base['是否新人'] = staff_base['first_online_day'].apply(lambda x: 'YES' if pd.isna(x) or ((x + pd.Timedelta(days=29)).date() >= pd.to_datetime(end_day).date()) else 'NO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日均外呼次数 日均通话时长\n",
    "staff_base['日均外呼次数'] = round(staff_base['call_times_sum'] / staff_base['attendance_sum'],0)\n",
    "staff_base['日均通话时长'] = round(staff_base['talk_duration_sum'] / staff_base['attendance_sum']/60,0)\n",
    "# 申完率 放款率\n",
    "\n",
    "# TODO 审完量放款量指标合并 (菲律宾特有)\n",
    "staff_base['申完量or放款量'] = np.where(staff_base['group'] == 'Telesales A', staff_base['application_sum'], staff_base['loan_sum'])\n",
    "\n",
    "# staff_base['申完率'] = round(staff_base['application_sum'] / staff_base['assign_sum'],4)\n",
    "# staff_base['放款率'] = round(staff_base['loan_sum'] / staff_base['assign_sum'],4)\n",
    "staff_base['申完率or放款率'] = round(staff_base['申完量or放款量'] / staff_base['assign_sum'],4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部员工进行排名\n",
    "'''\n",
    "1.排除上线天数等于0的不进行排名\n",
    "2.排除离职日期在当前日期之前并且上线天数小于等于15天的不进行排名\n",
    "'''\n",
    "all_rankings_staff = staff_base[~((staff_base['attendance_sum'] == 0) \n",
    "                | ((staff_base['attendance_sum'] <= 15)\n",
    "                    & (staff_base['dimission_date'].dt.date < pd.to_datetime(end_day).date())))]\n",
    "\n",
    "\n",
    "\n",
    "# 上线大于15天员工进行排名\n",
    "'''\n",
    "1.排除上线天数等于0的不进行排名\n",
    "2.排除离职日期在当前日期之前并且上线天数小于等于15天的不进行排名\n",
    "'''\n",
    "all_rankings_staff_15 = staff_base[(staff_base['attendance_sum'] > 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 员工排名区间算法\n",
    "def calculation_interval(row,column_name,target_column):\n",
    "    row[column_name]\n",
    "    max_Ranking = row[column_name]\n",
    "    if row[target_column] <= round(max_Ranking * 0.05,2):\n",
    "        return \"Top5%\"\n",
    "    elif row[target_column] <= round(max_Ranking * 0.25,2):\n",
    "        return \"5%-25%\"\n",
    "    elif row[target_column] <= round(max_Ranking * 0.5,2):\n",
    "        return \"25%-50%\"\n",
    "    elif row[target_column] <= round(max_Ranking * 0.7,2):\n",
    "        return \"50%-70%\"\n",
    "    elif row[target_column] <= round(max_Ranking * 0.9,2):\n",
    "        return \"70%-90%\"\n",
    "    else:\n",
    "        return \"bottom10%\"\n",
    "\n",
    "# 计算排名及排名区间\n",
    "def get_ranking_range(df):\n",
    "    # 审完率排名\n",
    "    df['申完率排名or放款率排名'] = df.groupby('group')['申完率or放款率'].rank(method='min', ascending=False)\n",
    "    # 统计参与排名总人数\n",
    "    df['参与排名总人数'] = df.groupby('group')['user_id'].transform('count')\n",
    "    # 计算排名区间\n",
    "    df['申完率排名区间or放款率区间'] = df.apply(lambda x: calculation_interval(x,'参与排名总人数','申完率排名or放款率排名'),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除离职且上线<=15的人员排名\n",
    "# 剔除未上线人员排名\n",
    "df_all = get_ranking_range(all_rankings_staff)\n",
    "df_all=df_all[['group','director','team_leader','user_id','申完率排名or放款率排名','申完率排名区间or放款率区间']]\n",
    "\n",
    "# 上线大于15天的人员的排名\n",
    "df_outpace_15 = get_ranking_range(all_rankings_staff_15)\n",
    "df_outpace_15.rename(columns={\n",
    "    '申完率排名or放款率排名':'申完率排名or放款率排名(大于15)',\n",
    "    '申完率排名区间or放款率区间':'申完率排名区间or放款率区间(大于15)'\n",
    "},inplace= True)\n",
    "# 选择特定列数据\n",
    "df_outpace_15 = df_outpace_15[['group','director','team_leader','user_id','申完率排名or放款率排名(大于15)','申完率排名区间or放款率区间(大于15)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  员工基表数据合并全部排名数据\n",
    "df_3 = pd.merge(staff_base,df_all,left_on=['group','director','team_leader','user_id'],right_on=['group','director','team_leader','user_id'],how='left')\n",
    "# 合并大于15天排名数据\n",
    "df_4 = pd.merge(df_3,df_outpace_15,left_on=['group','director','team_leader','user_id'],right_on=['group','director','team_leader','user_id'],how='left')\n",
    "\n",
    "# 判断员工业绩是否达标\n",
    "df_4['业绩是否达标'] = np.where(df_4['申完率排名区间or放款率区间'].isin(['Top5%','5%-25%','25%-50%']),'YES','NO')\n",
    "# 判断员工业绩是否达标\n",
    "df_4['业绩是否达标(大于15)'] = np.where((df_4['attendance_sum'] > 15) & (df_4['申完率排名区间or放款率区间(大于15)'].isin(['Top5%','5%-25%','25%-50%'])),'YES','NO')\n",
    "\n",
    "# 匹配员工工号及组长工号\n",
    "df_5 = pd.merge(df_4,no_count_churn_df.drop(columns=['name','count_churn']).rename(columns={'no':'员工工号'}),on='user_id',how='left')\n",
    "df_5 = pd.merge(df_5,no_count_churn_df.drop(columns=['user_id','count_churn']).rename(columns={'no':'组长工号'}),left_on='team_leader',right_on='name',how='left').drop(columns='name')\n",
    "df_5['应出勤天数'] = int(required_attendance_df['schedule_count'])\n",
    "员工数据_result = df_5\n",
    "# 员工数据计算 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ END ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组长数据计算 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ START ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# 计算员工在组长名下的上线天数\n",
    "组长名下员工上线天数=get_attendance_df.groupby(['group','director','team_leader','user_id']).agg(\n",
    "    组长名下上线天数 = ('attendance_status','sum')\n",
    ").reset_index()\n",
    "\n",
    "# 组长名下上线天数拼接到员工每日业绩里用于计算组长业绩\n",
    "组长每日业绩 = pd.merge(get_performance_df,组长名下员工上线天数,left_on=['group','director','team_leader','user_id'],right_on=['group','director','team_leader','user_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "组长业绩 = 组长每日业绩.groupby(['group','director','team_leader']).agg(\n",
    "    分案数 = ('include_assign','sum'),\n",
    "    申完数 = ('include_application','sum'),\n",
    "    放款数 = ('include_loan','sum'),\n",
    ").reset_index()\n",
    "\n",
    "组长业绩_大于15天 = 组长每日业绩[组长每日业绩['组长名下上线天数'] > 15].groupby(['group','director','team_leader']).agg(\n",
    "    分案数 = ('include_assign','sum'),\n",
    "    申完数 = ('include_application','sum'),\n",
    "    放款数 = ('include_loan','sum'),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计组长离职人数及总人数统计\n",
    "组长离职人数统计 = 员工数据_result.groupby(['group','director','team_leader'])['dimission_date'].count().reset_index().rename(columns={'dimission_date':'离职人数'})\n",
    "组长总人数统计 = 员工数据_result.groupby(['group','director','team_leader'])['user_id'].count().reset_index().rename(columns={'user_id':'总人数'})\n",
    "# 达标人数统计\n",
    "组长达标人数统计 = 员工数据_result[员工数据_result['业绩是否达标'] =='YES'].groupby(['group','director','team_leader'])['user_id'].count().reset_index().rename(columns={'user_id':'达标人数'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "组长业绩1=pd.merge(组长业绩,组长总人数统计,on=(['group','director','team_leader']),how='left')\n",
    "组长业绩2=pd.merge(组长业绩1,组长离职人数统计,on=(['group','director','team_leader']),how='left')\n",
    "组长业绩3=pd.merge(组长业绩2,组长达标人数统计,on=(['group','director','team_leader']),how='left')\n",
    "组长业绩4=pd.merge(组长业绩3,no_count_churn_df.drop(columns=['user_id','count_churn']).rename(columns={'name':'team_leader'}),on=('team_leader'),how='left').rename(columns={'no':'team_leader_no'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "组长业绩4['流失率'] = round(组长业绩4['离职人数']/ 组长业绩4['总人数'],4)\n",
    "组长业绩4['应达标人数'] =  组长业绩4['总人数']//2\n",
    "组长业绩4['申完数or放款数'] = np.where(组长业绩4['group']=='Telesales A',组长业绩4['申完数'],组长业绩4['放款数'])\n",
    "组长业绩4['申完率or放款率'] = round(组长业绩4['申完数or放款数'] / 组长业绩4['分案数'],4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 组长排名区间算法\n",
    "def calculation_interval_tl(row,column_name,target_column):\n",
    "    max_Ranking = row[column_name]\n",
    "    if row[target_column] <= round(max_Ranking * 0.2,2):\n",
    "        return \"TOP 20%\"\n",
    "    elif row[target_column] <= round(max_Ranking * 0.4,2):\n",
    "        return \"20%-40%\"\n",
    "    elif row[target_column] <= round(max_Ranking * 0.6,2):\n",
    "        return \"40%-60%\"\n",
    "    elif row[target_column] <= round(max_Ranking * 0.8,2):\n",
    "        return \"60%-80%\"\n",
    "    else:\n",
    "        return \"Bottom 20%\"\n",
    "    \n",
    "# 组长排名区间算法\n",
    "def calculation_interval_tl_1(row,target_column):\n",
    "    if row[target_column] == 1:\n",
    "        return \"20%-40%\"\n",
    "    elif row[target_column] == 2:\n",
    "        return \"40%-60%\"\n",
    "    elif row[target_column] == 3:\n",
    "        return \"60%-80%\"\n",
    "\n",
    "# 计算排名及排名区间\n",
    "def get_ranking_range_tl(df):\n",
    "    # 审完率排名\n",
    "    df['申完率排名or放款率排名'] = df.groupby('group')['申完率or放款率'].rank(method='min', ascending=False)\n",
    "    # 统计参与排名总人数\n",
    "    df['参与排名总人数'] = df.groupby('group')['team_leader_no'].transform('count')\n",
    "    # 计算排名区间\n",
    "    df['申完率排名区间or放款率区间'] = df.apply(lambda x: \n",
    "                                    calculation_interval_tl(x,'参与排名总人数','申完率排名or放款率排名')\n",
    "                                    if x['参与排名总人数'] > 3\n",
    "                                    else calculation_interval_tl_1(x,'申完率排名or放款率排名'),axis=1)\n",
    "\n",
    "    return df.drop(columns=('参与排名总人数'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "组长业绩_result=get_ranking_range_tl(组长业绩4)\n",
    "# 组长数据计算 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ END ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主管数据计算 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ START ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# 计算员工在主管名下的上线天数\n",
    "主管名下员工上线天数=get_attendance_df.groupby(['group','director','user_id']).agg(\n",
    "    主管名下上线天数 = ('attendance_status','sum')\n",
    ").reset_index()\n",
    "\n",
    "# 主管名下上线天数拼接到员工每日业绩里用于计算组长业绩\n",
    "主管每日业绩 = pd.merge(get_performance_df,主管名下员工上线天数,left_on=['group','director','user_id'],right_on=['group','director','user_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "主管业绩 = 主管每日业绩.groupby(['group','director']).agg(\n",
    "    分案数 = ('include_assign','sum'),\n",
    "    # 申完数 = ('include_application','sum'),\n",
    "    放款数 = ('include_loan','sum'),\n",
    ").reset_index()\n",
    "\n",
    "主管业绩_大于15天 = 主管每日业绩[主管每日业绩['主管名下上线天数'] > 15].groupby(['group','director','team_leader']).agg(\n",
    "    分案数 = ('include_assign','sum'),\n",
    "    # 申完数 = ('include_application','sum'),\n",
    "    放款数 = ('include_loan','sum'),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "主管离职人数统计 = 员工数据_result.groupby(['group','director'])['dimission_date'].count().reset_index().rename(columns={'dimission_date':'离职人数'})\n",
    "主管总人数统计 = 员工数据_result.groupby(['group','director'])['user_id'].count().reset_index().rename(columns={'user_id':'总人数'})\n",
    "# 达标人数统计\n",
    "主管达标人数统计 = 员工数据_result[员工数据_result['业绩是否达标'] =='YES'].groupby(['group','director'])['user_id'].count().reset_index().rename(columns={'user_id':'达标人数'})\n",
    "主管带组数 = 组长业绩_result.groupby(['group','director'])['team_leader'].count().reset_index().rename(columns={'team_leader':'带组数'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "主管业绩1=pd.merge(主管业绩,主管总人数统计,on=(['group','director']),how='left')\n",
    "主管业绩2=pd.merge(主管业绩1,主管离职人数统计,on=(['group','director']),how='left')\n",
    "主管业绩3=pd.merge(主管业绩2,主管达标人数统计,on=(['group','director']),how='left')\n",
    "主管业绩4=pd.merge(主管业绩3,no_count_churn_df.drop(columns=['user_id','count_churn']).rename(columns={'name':'director'}),on=('director'),how='left').rename(columns={'no':'director_no'})\n",
    "主管业绩5=pd.merge(主管业绩4,主管带组数,on=(['group','director']),how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "主管业绩5['流失率'] = round(主管业绩5['离职人数']/ 主管业绩4['总人数'],4)\n",
    "主管业绩5['应达标人数'] =  主管业绩5['总人数']//2\n",
    "主管业绩5['放款率'] = round(主管业绩5['放款数']/ 主管业绩4['分案数'],4)\n",
    "主管业绩_reult = 主管业绩5\n",
    "# 主管数据计算 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ END ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据排序排版 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ START ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "员工输出数据 = 员工数据_result[['group','director','组长工号','team_leader','员工工号','user_name','是否新人','first_online_day','dimission_date','日均外呼次数'\n",
    "             ,'日均通话时长','应出勤天数','attendance_sum','newly_sum','old_sum','申完量or放款量','申完率or放款率','申完率排名or放款率排名','申完率排名区间or放款率区间'\n",
    "             ,'业绩是否达标']]\n",
    "员工输出数据.columns = ['业务组','主管','组长工号','组长','坐席工号','坐席','是否新人','上线日期','离职日期','日均外呼次数'\n",
    "             ,'日均通话时长','应出勤天数','总上线天数','新人天数','老人天数','申完量or放款量','申完率or放款率','申完率排名or放款率排名','申完率排名区间or放款率区间'\n",
    "             ,'业绩是否达标']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 员工输出数据.columns = ['group', 'director', 'team_leader_no', 'team_leader', 'employee_no', 'user_name', 'is_new_employee', \n",
    "#                      'first_online_day', 'dimission_date', 'avg_daily_calls', 'avg_daily_call_duration', 'expected_attendance_days', \n",
    "#                      'attendance_sum', 'newly_sum', 'old_sum', 'application_or_loan_count', \n",
    "#                      'application_or_loan_rate', 'application_or_loan_rate_rank', 'application_or_loan_rate_range', \n",
    "#                      'is_performance_met']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "组长输出数据=组长业绩_result[['group','director','team_leader_no','team_leader','总人数','应达标人数','达标人数','离职人数','流失率','分案数','申完数or放款数','申完率or放款率','申完率排名or放款率排名','申完率排名区间or放款率区间']]\n",
    "组长输出数据.columns = ['业务组','主管','组长工号','组长','坐席数','应达标人数','实际达标人数','离职人数','流失率','分案数','申完数or放款数','申完率or放款率','申完率排名or放款率排名','申完率排名区间or放款率区间']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "主管输出数据 = 主管业绩_reult[['group', 'director_no','director','总人数','应达标人数', '达标人数', '离职人数', '流失率','分案数','放款数','放款率','带组数']]\n",
    "主管输出数据.columns=['业务组', '主管工号','主管','坐席数','应达标人数', '实际达标人数', '离职人数', '流失率','分案数','放款数' ,'放款率','带组数']\n",
    "# 数据排序排版 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ END ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./菲律宾电销/菲律宾电销日报(2024-09-21).xlsx 已成功美化\n"
     ]
    }
   ],
   "source": [
    "path = './菲律宾电销/菲律宾电销日报({0}).xlsx'.format(pd.to_datetime(end_day).date())\n",
    "ew = pd.ExcelWriter(path,engine='xlsxwriter')\n",
    "员工输出数据.to_excel(ew,sheet_name = '坐席数据',index = False)\n",
    "组长输出数据.to_excel(ew,sheet_name = '组长数据',index = False)\n",
    "主管输出数据.to_excel(ew,sheet_name = '主管数据',index = False)\n",
    "ew.close()\n",
    "beautify_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 发送邮件\n",
    "recipients = ['liufengfang@weidu.ac.cn']\n",
    "\n",
    "# recipients = config['philippines']['email']\n",
    "subject = '【菲律宾电销日报数据-{0}】'.format(pd.to_datetime(end_day).date())\n",
    "body =r\"\"\"<!DOCTYPE html><html><head><style>.indented {margin-left: 20px;}</style></head><body><p>各位好！</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;附件是菲律宾电销日报数据，请查收！谢谢！</p></body></html>\"\"\"\n",
    "send_bulk_emails_with_attachment(recipients, subject, body, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
